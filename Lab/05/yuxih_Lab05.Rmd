---
title: "yuxih_Lab05"
author: "Yuxin He"
date: "2023-11-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

### Exercise in spatial analysis

```{r}
library(terra)
#first import the land cover/use data 
NLCD_2001 <- rast("NLCD_2001_SL.tif")
NLCD_2004 <- rast("NLCD_2004_SL.tif")
NLCD_2006 <- rast("NLCD_2006_SL.tif")
NLCD_2008 <- rast("NLCD_2008_SL.tif")
NLCD_2011 <- rast("NLCD_2011_SL.tif")
NLCD_2013 <- rast("NLCD_2013_SL.tif")
NLCD_2016 <- rast("NLCD_2016_SL.tif")
# Distance to parks and protected areas in km (Euclidian) for the study area
Park_dist <- rast("Parks_dist_SL.tif")
# Road density for a 1 km neighborhood
Rd_dns1km <- rast("Rd_dns1km_SL.tif")
# Distance to water bodies in km (Euclidean)
WaterDist <- rast("WaterDist_SL.tif")
# elevation
DEM <- rast("DEM_SL.tif")
```

```{r}
plot(NLCD_2001)
```

```{r}
allrasters <- c(NLCD_2001, NLCD_2004, NLCD_2006, NLCD_2008, NLCD_2011, NLCD_2013, NLCD_2016, Park_dist, Rd_dns1km, WaterDist,DEM)
```

```{r}
#call single raster element
allrasters[[1]]
```

```{r}
#to run a function on an individual raster e.g., plot 
plot(allrasters[[1]])
```

```{r}
library(tidyverse)
```

```{r}
allrastersSL <- as.data.frame(allrasters, xy=TRUE)
## Here we are filtering out the no data values (stored as 128)
allrastersSL <- allrastersSL %>%
  filter (NLCD_2001_SL != 128)
head(allrastersSL)
```

### sampling

```{r}
library(leaflet)
sampleSLrnd <- spatSample(allrasters, size=100, "random", cells=TRUE, xy=TRUE)
head(sampleSLrnd)
```

```{r}
plot(sampleSLrnd$x, sampleSLrnd$y)
```

```{r}
sampleSLreg <- spatSample(allrasters, size=100,  "regular", cells=TRUE, xy=TRUE)
head(sampleSLreg)
```

```{r}
plot(sampleSLreg$x, sampleSLreg$y)
```

### Assessing our dataset
##Spatial autocorrelation

```{r}
# flatten the spatial data to a dataframe that has lat and long
flat_data <- as.data.frame(sampleSLrnd)
# calculate distances between all the points,    
dist_matrix <- as.matrix(dist(cbind(flat_data$x, flat_data$y)))
# and generate a matrix of inverse distance weights.
dist_matix.inv <- 1/dist_matrix
diag(dist_matix.inv) <- 0
```

```{r}
library(ape)
```

```{r}
Moran.I(sampleSLrnd$Rd_dns1km_SL, dist_matix.inv)
```

##statistical analysis

```{r}
allrastersSL <- allrastersSL %>%
    mutate(urbanChg = (NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) &  (NLCD_2016_SL == 21 | NLCD_2016_SL == 22  | NLCD_2016_SL == 23 | NLCD_2016_SL == 24)) 
```

```{r}
ggplot(allrastersSL, aes(y=y, x=x, color=urbanChg)) +
   geom_point(size=2, shape=15) +
   theme() 
```

```{r}
## calculate total new urban impervious for 2016
newUrban <- (sum(as.numeric(allrastersSL$NLCD_2016_SL == 21 | allrastersSL$NLCD_2016_SL == 22 |allrastersSL$NLCD_2016_SL == 23 | allrastersSL$NLCD_2016_SL == 24))) - (sum(as.numeric(allrastersSL$NLCD_2001_SL == 21| allrastersSL$NLCD_2001_SL == 22| allrastersSL$NLCD_2001_SL == 23| allrastersSL$NLCD_2001_SL == 24)))
## calculate total urban impervious for 2001
urban2001 <- (sum(as.numeric(allrastersSL$NLCD_2001_SL == 21| allrastersSL$NLCD_2001_SL == 22| allrastersSL$NLCD_2001_SL == 23| allrastersSL$NLCD_2001_SL == 24)))
## percentage increase in urban impervious
newUrban/urban2001* 100
```

```{r}
allrastersSL %>%
  filter(NLCD_2001_SL != 21| NLCD_2001_SL != 22| NLCD_2001_SL != 23| NLCD_2001_SL != 24) %>%
ggplot(aes(x=urbanChg, y=Parks_dist_SL)) + 
  geom_boxplot()
```

```{r}
library(plyr)
library(reshape2)
library(plotly)
```

```{r}
SL <- allrastersSL %>%
  filter(NLCD_2001_SL != 21 & NLCD_2001_SL != 22 & NLCD_2001_SL != 23 & NLCD_2001_SL != 24) 
SL <- SL[10:14]
SLmelt<-melt(SL)
```

```{r}
p <- ggplot(SLmelt, aes(x=urbanChg, y=value,fill=variable))+
    geom_boxplot()+
    facet_grid(.~variable)+
    labs(x="X (binned)")+
    theme(axis.text.x=element_text(angle=-90, vjust=0.4,hjust=1))
p
```

##General linear models

```{r}
###Grab all the developed cells (presence)
newUrban <- SL %>%
  filter(urbanChg == TRUE)

###Grab all the nondeveloped and not previously urban cells (absence)
nonUrban <- SL %>%
  filter(urbanChg == FALSE)

###Get a random sample of the absence data  
### that is twice as large as the presence data
index <- sample(1:nrow(nonUrban), (round(nrow(newUrban)* 2)))
SLsampleUrban <- nonUrban[index, ]

### combine the orginal presence and absence data
SLsample <- cbind(SLsampleUrban, newUrban)

###Consider a train and testing sample by futher subsampling the data
index <- sample(1:nrow(SL), (round(nrow(SL)* 0.01)))
SLsample <- SL[index, ]

###Consider making a training and testing dataset
###This can reduce the computational burden 
### It also is a robust goodness of fit method
#SLsample <- SLsample %>% dplyr::mutate(id = row_number())
#Create training set
#train <- SLsample %>% sample_frac(.70)
#Create test set
#test  <- anti_join(SLsample, train, by = 'id')
```

```{r}
fit <- glm(urbanChg ~ Parks_dist_SL + Rd_dns1km_SL + WaterDist_SL + DEM_SL,data=SLsample,family=binomial())
summary(fit)
```

### Goodness of fit
```{r}
## Loading required package: gplots
library(ROCR)
# plot a ROC curve for a single prediction run
# and color the curve according to cutoff.
pred <- prediction(predict(fit), SLsample$urbanChg)
perf <- performance(pred,"tpr","fpr")
plot(perf,colorize=TRUE)
```

```{r}
## A simpler way to understand these result is to calculate the
## area under the curve(AUC). The closer this number is to 1, the
## better your model fit
auc_ROCR <- performance(pred, measure = "auc")
auc_ROCR <- auc_ROCR@y.values[[1]]
auc_ROCR
```

### Using our model to predict likely locations of development
```{r}
predicted <- predict(allrasters, fit)
plot(predicted)
```

### Cluster analysis
```{r}
library(fastcluster)
library(graphics)
library(ggplot2)
library(ggdendro)
```

```{r}
library(sf)
```

```{r}
amenData<- st_read("AmenDataAll.shp")
```

```{r}
head(amenData)
```

```{r}
ggplot() + 
  geom_sf(data = amenData, mapping = aes(fill = cstdstL), color = NA) + 
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  theme(legend.position = "none") +
  ggtitle("Time to nearest urban Center (greater than 40000 people) in Seconds")
```

```{r}
st_geometry(amenData)
```

```{r}
geomData = st_sfc(amenData$geometry)
```

### Tranforming some data to make it meaningful and interpretable
##Geographic transformations
```{r}
amenData$ZooEmpDist_log <- log(amenData$ZooDistEmp + 0.00000001)
amenData$HotelEmpDist_log <- log(amenData$HotelDistE + 0.00000001)
amenData$HistMonDist_log <- log(amenData$HistMon_Di + 0.00000001)
amenData$HighEdEmpDist_log <- log(amenData$HigherEdDi + 0.00000001)
amenData$GolfEmpDist_log <- log(amenData$GolfDistEm + 0.00000001)
```

```{r}
amenData$SocialNorm <- amenData$Nat_Flickr/(amenData$serPop10 + 1)
amenData$HousingChg <- amenData$Urb2011 - amenData$Urb2001
```

##reduce to only the data that we need

```{r}
amenDataDF<-amenData[,c("SocialNorm", "HousingChg", "Frst2011", "WaterPct", "distcoast", "DEM_max","DEM_range", "HikeLength","ZooEmpDist_log", "HotelEmpDist_log","HistMonDist_log", "HighEdEmpDist_log", "GolfEmpDist_log")]
###if you want to add variable look at the names using: names(amenData)
```

```{r}
## make sure there are no missing data
amenDataDF <- na.omit(amenDataDF)
## we need to make it into a normal dataframe to 
## do the analysis
amenDataDF <- as.data.frame(amenDataDF)[1:12]
## calculate z-scores for the dataset
db_scaled <- scale(amenDataDF)
```

### K-means clustering
```{r}
# Determine number of clusters
wss <- (nrow(db_scaled)-1)*sum(apply(db_scaled,2,var))
for (i in 2:15) wss[i] <- sum(kmeans(db_scaled,
   centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
  ylab="Within groups sum of squares")
```

```{r}
# K-Means Cluster Analysis
fit <- kmeans(db_scaled, 5) # 5 cluster solution
```

```{r}
# get cluster means
aggregate(db_scaled,by=list(fit$cluster),FUN=mean)
```

```{r}
# append cluster assignment
amenData <- data.frame(amenData, fit$cluster)
```

```{r}
st_geometry(amenData) <- geomData
```

```{r}
ggplot() + 
  geom_sf(data = amenData, mapping = aes(fill = as.factor(fit.cluster)), color = NA) + 
  theme(legend.position = "none") +
  ggtitle("Clusters based on Kmeans")
```

### Hierarchical Clustering
```{r}
library(parallelDist)
##d <- parDist(db_scaled, method = "euclidean")
states <- st_read("states.shp")
```

```{r}
##Let's make sure these data have the same projection
states <- st_transform(states, st_crs(amenData))
##test the same variables as above

HCA <- amenData[,c("SocialNorm", "HousingChg", "Frst2011", "WaterPct", "distcoast", "DEM_max","DEM_range", "HikeLength","ZooEmpDist_log", "HotelEmpDist_log","HistMonDist_log", "HighEdEmpDist_log", "GolfEmpDist_log")]


##Now we can join them together, but we only need the name of states 
temp <- st_join(states[c("NAME", "geometry")], HCA)


###Now we can filter to the state of interest
Mich_scaled <- temp %>%
  dplyr::filter(NAME == "Michigan") %>%
  dplyr::select(SocialNorm, HousingChg, Frst2011, WaterPct, distcoast, DEM_max,DEM_range, HikeLength,ZooEmpDist_log, HotelEmpDist_log,HistMonDist_log, HighEdEmpDist_log, GolfEmpDist_log) %>%
  st_drop_geometry() %>% 
  scale()
```

```{r}
library(parallelDist)
d <- parDist(Mich_scaled, method = "euclidean")
```

```{r}
hclust.model <- hclust(na.omit(d), "ward.D")
## other option include "complete", "single", "average"
hclust.model
```

```{r}
hcd_ward <- as.dendrogram(hclust.model)
plot(hcd_ward, ylab = "Height", leaflab = "none")
```

```{r}
groups <- cutree(hclust.model, k=12) # cut tree into clusters
###Calculate means of variables based on cluster categories
means <- aggregate(Mich_scaled,by=list(groups),FUN=mean)
means 
```




